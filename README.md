# Coletor de Dados Supreme Posta

Automatiza√ß√£o de coleta de dados da plataforma Suprema Posta com envio autom√°tico para o Google BigQuery.

## üìã Vis√£o Geral

Este projeto foi desenvolvido para automatizar a coleta di√°ria de relat√≥rios da plataforma Suprema Posta. 
O sistema realiza login autom√°tico, baixa os relat√≥rios dispon√≠veis e os envia para tabelas estruturadas no Google BigQuery.

### Agendamento Autom√°tico

O sistema est√° configurado para executar automaticamente todos os dias √†s 00:15 (hor√°rio do servidor), garantindo a coleta dos dados do dia anterior. Este hor√°rio foi escolhido para:
- Garantir que todos os dados do dia anterior estejam dispon√≠veis
- Evitar hor√°rios de pico da plataforma
- Permitir tempo suficiente para retry em caso de falhas

### Relat√≥rios Coletados

- Relat√≥rio de M√≠dia
- Relat√≥rio de Registros
- Relat√≥rio de Ganhos
- Relat√≥rio de Atividades

## üõ†Ô∏è Tecnologias Utilizadas

- Python 3.9
- Selenium (automa√ß√£o web)
- Google BigQuery (armazenamento de dados)
- Docker (containeriza√ß√£o)
- Chromium (navegador headless)

## ‚öôÔ∏è Pr√©-requisitos

- Docker e Docker Compose
- Conta no Google Cloud Platform com BigQuery ativado
- Credenciais de acesso ao Supreme Posta
- Acesso √† internet

## üöÄ Configura√ß√£o e Instala√ß√£o

### 1. Prepara√ß√£o do Ambiente Google Cloud

1. Acesse o [Console do Google Cloud](https://console.cloud.google.com/)
2. Crie ou selecione um projeto
3. Ative a API do BigQuery
4. Crie uma conta de servi√ßo:
   - V√° em "IAM e Administra√ß√£o" > "Contas de servi√ßo"
   - Clique em "Criar Conta de Servi√ßo"
   - Nome: `supreme-posta-scraper`
   - Atribua as fun√ß√µes:
     * `BigQuery Data Editor`
     * `BigQuery Job User`
   - Crie uma chave em formato JSON
   - Salve como `gcp-key.json`

### 2. Configura√ß√£o Local

1. Clone o reposit√≥rio:
```bash
git clone [URL_DO_REPOSIT√ìRIO]
cd [NOME_DO_DIRET√ìRIO]
```

2. Crie o arquivo `.env`:
```env
# Credenciais Supreme Posta
USERNAME=seu_usuario
PASSWORD=sua_senha

# Configura√ß√µes Google Cloud
GCP_PROJECT_ID=seu-projeto-gcp
BQ_DATASET_ID=seu-dataset
```

3. Coloque o arquivo `gcp-key.json` na raiz do projeto

### 3. Cria√ß√£o do Dataset

No BigQuery, crie o dataset que receber√° os dados:

```sql
CREATE DATASET seu-projeto-gcp.seu-dataset
```

## üì¶ Execu√ß√£o

Para iniciar a coleta:

```bash
docker-compose up --build
```

O sistema ir√°:
1. Realizar login na plataforma
2. Baixar os relat√≥rios do dia anterior
3. Processar os dados
4. Enviar para o BigQuery

## üìä Estrutura dos Dados

Os dados s√£o organizados nas seguintes tabelas:

| Tabela                 | Descri√ß√£o                  |
| ---------------------- | -------------------------- |
| `relatorio_midia`      | Dados de m√≠dia e campanhas |
| `relatorio_registros`  | Registros de atividades    |
| `relatorio_ganhos`     | Informa√ß√µes financeiras    |
| `relatorio_atividades` | Atividades gerais          |

## üìù Logs e Monitoramento

- Localiza√ß√£o: diret√≥rio `logs/`
- Rota√ß√£o: di√°ria
- Reten√ß√£o: 7 dias
- Informa√ß√µes registradas:
  * Status de login
  * Download de relat√≥rios
  * Processamento de dados
  * Envio para BigQuery
  * Erros e exce√ß√µes

## üîí Seguran√ßa

### Boas Pr√°ticas

1. Nunca compartilhe ou comite:
   - Arquivo `.env`
   - `gcp-key.json`
   - Logs
   - Arquivos tempor√°rios

2. O `.gitignore` j√° est√° configurado para proteger:
   - Credenciais (*.json)
   - Vari√°veis de ambiente (.env)
   - Logs e tempor√°rios

3. Recomenda√ß√µes:
   - Use gerenciamento de segredos em produ√ß√£o
   - Fa√ßa rota√ß√£o peri√≥dica das chaves
   - Mantenha as permiss√µes m√≠nimas necess√°rias

## üîç Solu√ß√£o de Problemas

Se encontrar problemas:

1. Verifique os logs em `logs/`
2. Confirme as credenciais no `.env`
3. Valide as permiss√µes no Google Cloud
4. Verifique a conex√£o com internet
5. Confirme se o Docker est√° atualizado

## ü§ù Contribuindo

1. Fa√ßa um fork do projeto
2. Crie uma branch para sua feature
   ```bash
   git checkout -b feature/nova-funcionalidade
   ```
3. Commit suas altera√ß√µes
   ```bash
   git commit -m 'Adiciona nova funcionalidade'
   ```
4. Push para a branch
   ```bash
   git push origin feature/nova-funcionalidade
   ```
5. Abra um Pull Request

## üì´ Suporte

Em caso de d√∫vidas ou problemas:
- Abra uma issue no reposit√≥rio
- Consulte a documenta√ß√£o do [Google BigQuery](https://cloud.google.com/bigquery/docs)
- Verifique os logs de execu√ß√£o

## ‚ú® Agradecimentos

Agradecimento especial ao [JP](https://github.com/JoaoPSilvaDev) pela colabora√ß√£o e conhecimento compartilhado durante o desenvolvimento deste projeto.

## üìÑ Licen√ßa

Este projeto est√° sob a licen√ßa MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.

# Descri√ß√£o

Um amigo pediu-me que eu desenvolvesse uma aplica√ß√£o 
capaz de realiziar WebScrapping em um determinado site,
al√©m de desenvolver uma forma de execut√°-lo com periodicidade.

Sendo assim, desenvolvi este conjunto de programas que 
alcan√ßam essas necessidades.

## Tecnologias Utilizadas

* PostGreSQL
* Selenium
* Google BigQuery
* Docker
* Python

## Configura√ß√£o das Credenciais

### 1. Criar uma Conta de Servi√ßo no Google Cloud (Recomendado para Produ√ß√£o)

1. Acesse o [Console do Google Cloud](https://console.cloud.google.com/)
2. Selecione seu projeto
3. V√° para "IAM e Administra√ß√£o" > "Contas de servi√ßo"
4. Clique em "Criar Conta de Servi√ßo"
5. Preencha os detalhes:
   - Nome: `supreme-posta-scraper`
   - Descri√ß√£o: `Conta de servi√ßo para coleta de dados do Supreme Posta`
6. Conceda estas fun√ß√µes:
   - `BigQuery Data Editor`
   - `BigQuery Job User`
7. Clique em "Criar Chave" (formato JSON)
8. Salve o arquivo JSON como `gcp-key.json` na raiz do projeto

### 2. Configurar Vari√°veis de Ambiente

Crie um arquivo `.env` na raiz do projeto com:

```env
# Credenciais do Supreme Posta
USERNAME=seu_usuario_supreme_posta
PASSWORD=sua_senha_supreme_posta

# Configura√ß√µes do Google Cloud
GCP_PROJECT_ID=seu-projeto-id
BQ_DATASET_ID=nome-do-seu-dataset
```

### 3. Criar Dataset no BigQuery

1. Acesse o BigQuery no Console do Google Cloud
2. Crie um novo dataset:
   ```sql
   CREATE DATASET seu-projeto-id.nome-do-seu-dataset
   ```

### 4. Executando com Docker

Para executar o projeto:

```bash
# Construir e iniciar os containers
docker-compose up --build
```

### 5. Requisitos do Sistema

* Docker e Docker Compose instalados
* Acesso √† internet
* Credenciais do Suprema Posta v√°lidas
* Projeto configurado no Google Cloud com BigQuery habilitado

### Notas de Seguran√ßa

1. Nunca comite a chave de conta de servi√ßo no controle de vers√£o
2. O arquivo `.gitignore` j√° est√° configurado para ignorar:
   - Arquivos de credenciais (*.json)
   - Vari√°veis de ambiente (.env)
   - Logs e arquivos tempor√°rios
3. Use gerenciamento de segredos em produ√ß√£o
4. Fa√ßa rota√ß√£o peri√≥dica das chaves de conta de servi√ßo
5. Siga o princ√≠pio do menor privil√©gio ao atribuir fun√ß√µes

### Estrutura de Dados

Os dados s√£o coletados e enviados para tabelas espec√≠ficas no BigQuery:

1. `relatorio_midia` - Relat√≥rios de m√≠dia
2. `relatorio_registros` - Relat√≥rios de registros
3. `relatorio_ganhos` - Relat√≥rios de ganhos
4. `relatorio_atividades` - Relat√≥rios de atividades

### Logs e Monitoramento

Os logs s√£o armazenados no diret√≥rio `logs/` e incluem:
- Informa√ß√µes de execu√ß√£o
- Erros e exce√ß√µes
- Status das coletas
- Confirma√ß√µes de envio para o BigQuery

### Suporte

Em caso de d√∫vidas ou problemas:
1. Verifique os logs em `logs/`
2. Confirme as credenciais no arquivo `.env`
3. Verifique as permiss√µes da conta de servi√ßo no Google Cloud
4. Certifique-se de que o Docker est√° atualizado e funcionando corretamente
